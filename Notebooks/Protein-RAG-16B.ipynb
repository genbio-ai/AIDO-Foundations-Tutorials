{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd9813ed",
   "metadata": {},
   "source": [
    "# AIDO.Protein-RAG-16B\n",
    "\n",
    "[AIDO.Protein-RAG-16B](https://huggingface.co/genbio-ai/AIDO.Protein-RAG-16B) is a multimodal protein language model that integrates Multiple Sequence Alignment (MSA) and structural data, building upon the AIDO.Protein-16B foundation. The training process comprises three main stages:\n",
    "\n",
    "1. 2D RoPE encoding fine-tuning\n",
    "2. Initial training on 100 billion tokens from UniRef50/UniClust30 MSA data\n",
    "3. Subsequent training on 80 billion tokens from AlphaFold Database MSA and structural data\n",
    "\n",
    "<img src=\"images/rag_1.png\" alt=\"AIDO.Protein-RAG\" width=\"300\" style=\"background-color:white;\"/>\n",
    "\n",
    "<img src=\"images/rag_2.png\" alt=\"AIDO.Protein-RAG\" width=\"400\" style=\"background-color:white;\"/>\n",
    "\n",
    "| Hyper-params                | (1) 1D -> 2D finetuning | (2) UniRef50/Uniclust30 MSA finetuning | (3) AFDB MSA & Structure tokens finetuning |\n",
    "| --------------------------- | :---------------------: | :------------------------------------: | :----------------------------------------: |\n",
    "| Initialized parameters      |   AIDO.Protein-16B      |       Stage (1)                        |                      Stage (2)             |\n",
    "| Data                        |   ColabFoldDB, UniRef   |       HHblits_MSA, Retriever_MSA       |        AFDB MSA & Structure tokens         |\n",
    "| Global Batch Size           |           512           |                  256                   |                    256                     |\n",
    "| Sequence length             |          2048           |                 12800                  |                   12800                    |\n",
    "| Per Device Micro Batch Size |            1            |                   1                    |                     1                      |\n",
    "| Precision                   |     Mixed FP32-FP16     |            Mixed FP32-FP16             |              Mixed FP32-FP16               |\n",
    "| LR                          |       [5e-6,5e-5]       |              [1e-6, 1e-5]              |                    1e-5                    |\n",
    "| Num Tokens                  |       10 billion        |              100 billion               |                 80 billion                 |\n",
    "\n",
    "Reference: [Retrieval Augmented Protein Language Models for Protein Structure Prediction](https://www.biorxiv.org/content/10.1101/2024.12.02.626519v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc5eb9f",
   "metadata": {},
   "source": [
    "## Step-by-Step Example\n",
    "\n",
    "I will introduce how to manually load the model and tokenizer; how to preprocess the input MSA file and PDB file; and finally how to obtain the protein embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9e3b2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n",
      "modelgenerator_path: /jfs/pan-li/Demo/ModelGenerator/modelgenerator\n",
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello world\")\n",
    "import os, sys, pathlib, torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "os.environ['HF_HOME'] = '/tmp/hf_cache'\n",
    "\n",
    "import modelgenerator\n",
    "modelgenerator_path = str(pathlib.Path(modelgenerator.__file__).parent)\n",
    "print(f\"modelgenerator_path: {modelgenerator_path}\")\n",
    "\n",
    "from modelgenerator.huggingface_models.fm4bio import FM4BioForMaskedLM\n",
    "from modelgenerator.huggingface_models.fm4bio import FM4BioTokenizer\n",
    "print(\"Hello world\")\n",
    "\n",
    "from utils import misc\n",
    "from utils import protein"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f91d3c",
   "metadata": {},
   "source": [
    "#### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d38de41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/genbio/lib/python3.12/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 13/13 [00:08<00:00,  1.59it/s]\n",
      "Some weights of FM4BioForMaskedLM were not initialized from the model checkpoint at genbio-ai/AIDO.Protein-RAG-16B and are newly initialized: ['output_embed.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FM4BioForMaskedLM(\n",
      "  (bert): FM4BioModel(\n",
      "    (embeddings): FM4BioEmbeddings(\n",
      "      (word_embeddings): Embedding(640, 2304, padding_idx=0)\n",
      "      (str_embeddings): Linear(in_features=384, out_features=2304, bias=False)\n",
      "      (dropout): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (encoder): FM4BioEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-35): 36 x FM4BioLayer(\n",
      "          (attention): FM4BioAttention(\n",
      "            (ln): RnaRMSNorm()\n",
      "            (self): FM4BioSelfAttention(\n",
      "              (query): Linear(in_features=2304, out_features=2304, bias=True)\n",
      "              (key): Linear(in_features=2304, out_features=2304, bias=True)\n",
      "              (value): Linear(in_features=2304, out_features=2304, bias=True)\n",
      "              (dropout): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "            (output): FM4BioSelfOutput(\n",
      "              (dense): Linear(in_features=2304, out_features=2304, bias=True)\n",
      "              (dropout): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (ln): RnaRMSNorm()\n",
      "          (mlp): FM4BioMLP(\n",
      "            (router): Linear(in_features=2304, out_features=8, bias=False)\n",
      "            (dense_h_to_4h_0): Linear(in_features=2304, out_features=15360, bias=True)\n",
      "            (dense_4h_to_h_0): Linear(in_features=7680, out_features=2304, bias=True)\n",
      "            (dense_h_to_4h_1): Linear(in_features=2304, out_features=15360, bias=True)\n",
      "            (dense_4h_to_h_1): Linear(in_features=7680, out_features=2304, bias=True)\n",
      "            (dense_h_to_4h_2): Linear(in_features=2304, out_features=15360, bias=True)\n",
      "            (dense_4h_to_h_2): Linear(in_features=7680, out_features=2304, bias=True)\n",
      "            (dense_h_to_4h_3): Linear(in_features=2304, out_features=15360, bias=True)\n",
      "            (dense_4h_to_h_3): Linear(in_features=7680, out_features=2304, bias=True)\n",
      "            (dense_h_to_4h_4): Linear(in_features=2304, out_features=15360, bias=True)\n",
      "            (dense_4h_to_h_4): Linear(in_features=7680, out_features=2304, bias=True)\n",
      "            (dense_h_to_4h_5): Linear(in_features=2304, out_features=15360, bias=True)\n",
      "            (dense_4h_to_h_5): Linear(in_features=7680, out_features=2304, bias=True)\n",
      "            (dense_h_to_4h_6): Linear(in_features=2304, out_features=15360, bias=True)\n",
      "            (dense_4h_to_h_6): Linear(in_features=7680, out_features=2304, bias=True)\n",
      "            (dense_h_to_4h_7): Linear(in_features=2304, out_features=15360, bias=True)\n",
      "            (dense_4h_to_h_7): Linear(in_features=7680, out_features=2304, bias=True)\n",
      "          )\n",
      "          (output): FM4BioOutput(\n",
      "            (dropout): Dropout(p=0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ln): RnaRMSNorm()\n",
      "    )\n",
      "    (rotary_pos_emb): RotaryEmbedding()\n",
      "  )\n",
      "  (output_embed): Linear(in_features=2304, out_features=640, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = FM4BioForMaskedLM.from_pretrained(\"genbio-ai/AIDO.Protein-RAG-16B\")\n",
    "model = model.cuda().eval()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c44bf3",
   "metadata": {},
   "source": [
    "#### Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c7d046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = os.path.join(modelgenerator_path, \"huggingface_models/fm4bio/vocab_protein.txt\")\n",
    "tokenizer = FM4BioTokenizer(vocab_file=vocab_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc79886",
   "metadata": {},
   "source": [
    "#### Load MSA and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed8c8d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of MSA sequences: 82905\n",
      "MAAAAAAGAGPEMVRGQVFDVGPRYTNLSYIGEGAYGMVCSAYDNVNKVRVAIKKISPFEHQTYCQRTLREIKILLRFRHENIIGINDIIRAPTIEQMKDVYIVQDLMETDLYKLLKTQHLSNDHICYFLYQILRGLKYIHSANVLHRDLKPSNLLLNTTCDLKICDFGLARVADPDHDHTGFLTEYVATRWYRAPEIMLNSKGYTKSIDIWSVGCILAEMLSNRPIFPGKHYLDQLNHILGILGSPSQEDLNCIINLKARNYLLSLPHKNKVPWNRLFPNADSKALDLLDKMLTFNPHKRIEVEQALAHPYLEQYYDPSDEPIAEAPFKFDMELDDLPKEKLKELIFEETARFQPGYRS\n",
      "-------------VRGQVFDVGPRYTNLSYIGEGAYGMVCSAYDNVNKVRVAIKKISPFEHQTYCQRTLREIKILLRFRHENIIGINDIIRAPTIEQMKDVYIVQDLMETDLYKLLKTQHLSNDHICYFLYQILRGLKYIHSANVLHRDLKPSNLLLNTTCDLKICDFGLARVADPDHDHTGFLTEYVATRWYRAPEIMLNSKGYTKSIDIWSVGCILAEMLSNRPIFPGKHYLDQLNHILGILGSPSQEDLNCIINLKARNYLLSLPHKNKVPWNRLFPNADSKALDLLDKMLTFNPHKRIEVEQALAHPYLEQYYDPSDEPIAEAPFKFDMELDDLPKEKLKELIFEETARFQPG---\n",
      "-------------VRGQVFDVGPRYTNLSYIGEGAYGMVCSAYDNINKVRVAIKKISPFEHQTYCQRTLREIKILLRFRHENIIGINDIIRAPTIEQMKDVYIVQDLMETDLYKLLKTQHLSNDHICYFLYQILRGLKYIHSANVLHRDLKPSNLLLNTTCDLKICDFGLARVADPDHDHTGFLTEYVATRWYRAPEIMLNSKGYTKSIDIWSVGCILAEMLSNRPIFPGKHYLDQLNHILGILGSPSQEDLNCIINLKARNYLLSLPHKNKVPWNRLFPNADSKALDLLDKMLTFNPHKRIEVEQALAHPYLEQYYDPSDEPIAEAPFKFDMELDDLPKEKLKELIFEETARFQPG---\n",
      "-------------VRGQVFDVGPRYTNLSYIGEGAYGM-CSAYDNVNKVRVAIKKISPFEHQTYCQRTLREIKILLRFRHENIIGINDIIRAPTIEQMKDVYIVQDLMETDLYKLLKTQHLSNDHICYFLYQILRGLKYIHSANVLHRDLKPSNLLLNTTCDLKICDFGLARVADPDHDHTGFLTEYVATRWYRAPEIMLNSKGYTKSIDIWSVGCILAEMLSNRPIFPGKHYLDQLNHILGILGSPSQEDLNCIINLKARNYLLSLPHKNKVPWNRLFPNADSKALDLLDKMLTFNPHKRIEVEQALAHPYLEQYYDPSDEPIAEAPFKFDMELDDLPKEKLKELIFEETARFQPG---\n",
      "-------------VRGQVFDVGPRYTNLSYIGEGAYGMVWSAYDNVNKVRVAIKKISPFEHQTYCQRTLREIKILLRFRHENIIGINDIIRAPTIEQMKDVYIVQDLMETDLYKLLKTQHLSNDHICYFLYQILRGLKYIHSANVLHRDLKPSNLLLNTTCDLKICDFGLARVADPDHDHTGFLTEYVATRWYRAPEIMLNSKGYTKSIDIWSVGCILAEMLSNRPIFPGKHYLDQLNHILGILGSPSQEDLNCIINLKARNYLLSLPHKNKVPWNRLFPNADSKALDLLDKMLTFNPHKRIEVEQALAHPYLEQYYDPSDEPIAEAPFKFDMELDDLPKEKLKELIFEETARFQPG---\n",
      "length of query: 360\n"
     ]
    }
   ],
   "source": [
    "msa = misc.load_msa_txt(\"sample_data/MK01_HUMAN_Brenan_2016.txt.gz\")\n",
    "print(f\"Number of MSA sequences: {len(msa)}\")\n",
    "for seq in msa[:5]:\n",
    "    print(seq)\n",
    "\n",
    "query_sequence = msa[0]\n",
    "print(f\"length of query: {len(query_sequence)}\")\n",
    "msa = msa[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c747770",
   "metadata": {},
   "source": [
    "If we concatenate all MSA tokens, it will be 82905 $\\times$ 360 = 29,845,800 tokens ! We use the greedy select strategy to obtain a subset from all MSAs so that the edit distance between each sequence is maximized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bc63093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of MSA sequences: 43\n",
      "---------NKQTVQNHSFDTIDKYKVTEIIGSGTYGVVAICKELQTDQQFALKKNIVFPDENHQLRMLRELKMLHHFRCPYIVNLKDVYVPNQLNQLRDIEMITSLMEADLRDIFDSQSLSPKHVKWFMYQICLSVYYMQKAKILHRDLKPENILVNSQCDVAICDFGLARGYYNSLQNKKLSSNYVVSRWYRPPELLTNATQYNKTLDMWSVGCIMYELLKGEVLFKGSGSIDQIQRIIKQLGTPAIDDFNGSEAARDYIY-NKFPICKRRSFTKRLPNTCPMAIDLMKRMLTFNMYKRINPLDALLHPYFREFYDQSDLNIPLSPFDTAWEENMLSSNDLKLEAFNTLKSIKK----\n",
      "----------VYKVRGQSFDIEDTYTVTSVVGHGAYGVVCAALDDRTFQEVAIKRVSVFEDLIDGRRIWREILILRLLRCRNMLRLLRVLPPKPITEFRDLYMVTDLFDTDLFAIIRQKNMSTDMLRRVGARVLQCLADMHTMGIVHRDIKPSNILLRDEENATVCDFGLARAGLLDLTEPLDLTDYVVTRWYRPPELLLM-CSYSFPIDMWAVGCVMAEYVTQRPIFAGRDYIHQLQLVLASVNITGTSFMESTSASAINHMNDVARKYGTRPLSNLLAALPKEGFDLVNRMLAFEPDNRITALEALQHPFFEPLALEEPARTLSPAVELSFDMAEISEYQLRRAIWDEVEHYKKQ---\n",
      "---------NTVEHSLTQFTVPKRYQNLSPFEYNSHDIVTYATDTNTGKKVTIRKILPFDSVARAHRTYRELKLRIHLNDAQVAQLYDVFTPEDLNNFETLYLVENYVEYDLKRVIYSVVVTEDHIKMVIYCLLRGLKFIHSAGIIHSRIISSNIGIDKDSNVSIFGWDSAATAHIRRKYDEYNESDIYRRWY-APEMIINPEHCNEKVDIWSVGCIMAELIVRQPLFPGTSQSDQLTKIFDITGTPDSKTLDEMNMPSVRQYCETLSRKSKQDYEKLFGYVSSQGVTLLDRLLLLNHCMRPTAEELLNDPYFEMYHDPIDEPSSE-LLIDEYQDATYSTEKWKSITWNMVKEFVP----\n",
      "-------------MSSTEFIVDARYRNMKLLDSGSYGIVVRAFDSKTKTAVAIKKIFAFSCSRMARHVLREVRLLRHLDHPHIVKLLDIDVPGQYTAWQSVYIVTPLLEVDLYTAFRQKVDDVLKQKKIAYQLLLALEHMHSHGLMHRDIKSKNLLLDKDMNVQLCDLGESRFYSKANELEPQLSGVITTILQSAPELVFG-DEYDAEVDIWAAGCVIAEIVRHEYLFDSVAKHSHLQEIVDIIGYPTPDVAETIDKN-AAWVLRRVRKSSGNRIADMLPSVDQLAIDLVEKMLTFSPKQRISATDALHHPWFDEARDETINNRGKYDFAQTEPSKKTPKSKLKRLVWDEIIGFHPE---\n",
      "------QQFVQSDVSGSTFETPSRYTDLCVVDSRVRDNTCTAFDRLCRQPVTVKRINPFDRIEAVRYAYQQVRLAGELQHDNLVFLQDIYISP----TEDIYLITELHGIPLGGLLKDGPLDPALVQFLMYQILRGLKYIHSAGVVHGDLTPSKLFVDRNCDLKLSDFHIDRETDPN-------GTDHTSLHYEAPEIMLAWSLYDMGVDIFSAGCIFAEMMLGKPLFPSKDHAQQLGFATGLLGPPPQWMVQNSSI--AKINSRSWSGYQPRRISSIFAQQDPEAVDLLDSMLMYDPRERITADNAITHPYLERYHDASDEPVAVQKFDWTFKDAQIPGATWKWMIYSEATRYHTE---\n",
      "Number of tokens in MSA (excluding gaps): 13333\n"
     ]
    }
   ],
   "source": [
    "f_msa = misc.greedy_select(msa, num_seqs=None, num_tokens=12_800, seed=0)\n",
    "f_msa.sort(key=lambda x: x.count('-'))\n",
    "\n",
    "print(f\"Number of MSA sequences: {len(f_msa)}\")\n",
    "for seq in f_msa[:5]:\n",
    "    print(seq)\n",
    "\n",
    "num_tokens = sum([ len(seq)-seq.count('-') for seq in f_msa ])\n",
    "print(f\"Number of tokens in MSA (excluding gaps): {num_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41da59b3",
   "metadata": {},
   "source": [
    "#### Load Structure Tokenizer, PDB file and get structure embeddings\n",
    "\n",
    "We used [genbio-ai/AIDO.StructureTokenizer](https://huggingface.co/genbio-ai/AIDO.StructureTokenizer) to tokenize the protein structure into discrete tokens and embeddings.\n",
    "\n",
    "<img src=\"images/aido_structure.png\" alt=\"aido_structure\" width=\"800\" style=\"background-color:white;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efb9b705",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_tokenizer = misc.AIDO_Structure_Tokenizer(device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f92ce5f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Molecular weight: 38.433KDa\n",
       "A (360): MAAAAAAGAGPEMVRGQVFDVGPRYTNLSYIGEGAYGMVCSAYDNVNKVRVAIKKISPFEHQTYCQRTLREIKILLRFRHENIIGINDIIRAPTIEQMKDVYIVQDLMETDLYKLLKTQHLSNDHICYFLYQILRGLKYIHSANVLHRDLKPSNLLLNTTCDLKICDFGLARVADPDHDHTGFLTEYVATRWYRAPEIMLNSKGYTKSIDIWSVGCILAEMLSNRPIFPGKHYLDQLNHILGILGSPSQEDLNCIINLKARNYLLSLPHKNKVPWNRLFPNADSKALDLLDKMLTFNPHKRIEVEQALAHPYLEQYYDPSDEPIAEAPFKFDMELDDLPKEKLKELIFEETARFQPGYRS"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(f\"sample_data/MK01_HUMAN_Brenan_2016.pdb\") as IN:\n",
    "    text = IN.read()\n",
    "\n",
    "prot = protein.from_pdb_string(text, molecular_type='protein')\n",
    "prot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cca7f7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prot.aatype: (360,)\n",
      "[12  0  0  0  0  0  0  7  0  7 14  6 12 19  1  7  5 19 13  3 19  7 14  1\n",
      " 18 16  2 10 15 18  9  7  6  7  0 18  7 12 19  4 15  0 18  3  2 19  2 11\n",
      " 19  1 19  0  9 11 11  9 15 14 13  6  8  5 16 18  4  5  1 16 10  1  6  9\n",
      " 11  9 10 10  1 13  1  8  6  2  9  9  7  9  2  3  9  9  1  0 14 16  9  6\n",
      "  5 12 11  3 19 18  9 19  5  3 10 12  6 16  3 10 18 11 10 10 11 16  5  8\n",
      " 10 15  2  3  8  9  4 18 13 10 18  5  9 10  1  7 10 11 18  9  8 15  0  2\n",
      " 19 10  8  1  3 10 11 14 15  2 10 10 10  2 16 16  4  3 10 11  9  4  3 13\n",
      "  7 10  0  1 19  0  3 14  3  8  3  8 16  7 13 10 16  6 18 19  0 16  1 17\n",
      " 18  1  0 14  6  9 12 10  2 15 11  7 18 16 11 15  9  3  9 17 15 19  7  4\n",
      "  9 10  0  6 12 10 15  2  1 14  9 13 14  7 11  8 18 10  3  5 10  2  8  9\n",
      " 10  7  9 10  7 15 14 15  5  6  3 10  2  4  9  9  2 10 11  0  1  2 18 10\n",
      " 10 15 10 14  8 11  2 11 19 14 17  2  1 10 13 14  2  0  3 15 11  0 10  3\n",
      " 10 10  3 11 12 10 16 13  2 14  8 11  1  9  6 19  6  5  0 10  0  8 14 18\n",
      " 10  6  5 18 18  3 14 15  3  6 14  9  0  6  0 14 13 11 13  3 12  6 10  3\n",
      "  3 10 14 11  6 11 10 11  6 10  9 13  6  6 16  0  1 13  5 14  7 18  1 15]\n",
      "prot.atom_positions: (360, 37, 3)\n",
      "[[[-30.01600075 -10.79699993  49.46900177]\n",
      "  [-29.79700089 -10.97700024  48.03099823]\n",
      "  [-29.          -9.81200027  47.46900177]\n",
      "  ...\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]]\n",
      "\n",
      " [[-27.75        -9.66399956  48.        ]\n",
      "  [-26.56200027  -8.84399986  47.84400177]\n",
      "  [-26.09399986  -8.83600044  46.375     ]\n",
      "  ...\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]]\n",
      "\n",
      " [[-26.59399986  -7.92999983  45.5       ]\n",
      "  [-26.07799911  -7.21899986  44.31200027]\n",
      "  [-24.59399986  -6.92600012  44.46900177]\n",
      "  ...\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-25.85899925   5.47300005  19.        ]\n",
      "  [-25.90600014   6.28100014  17.79700089]\n",
      "  [-27.18799973   6.01599979  17.01600075]\n",
      "  ...\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]]\n",
      "\n",
      " [[-27.96899986   7.08199978  16.82799911]\n",
      "  [-29.15600014   7.0079999   16.        ]\n",
      "  [-28.93799973   7.68400002  14.64799976]\n",
      "  ...\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]]\n",
      "\n",
      " [[-29.01600075   7.02299976  13.47700024]\n",
      "  [-28.81200027   7.48000002  12.10200024]\n",
      "  [-29.71899986   8.65600014  11.77299976]\n",
      "  ...\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]]]\n",
      "prot.atom_mask: (360, 37)\n",
      "[[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"prot.aatype: {prot.aatype.shape}\\n{prot.aatype}\")\n",
    "print(f\"prot.atom_positions: {prot.atom_positions.shape}\\n{prot.atom_positions}\")\n",
    "print(f\"prot.atom_mask: {prot.atom_mask.shape}\\n{prot.atom_mask}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd469943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([360, 384])\n",
      "torch.Size([360])\n"
     ]
    }
   ],
   "source": [
    "str_embs, str_toks = str_tokenizer.encode(prot.aatype, prot.atom_positions, prot.atom_mask, get_embedding=True)\n",
    "str_embs = str_embs.cuda()\n",
    "str_toks = str_toks.cuda()\n",
    "print(str_embs.shape)\n",
    "print(str_toks.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26632be",
   "metadata": {},
   "source": [
    "#### Tokenize the protein sequences\n",
    "\n",
    "<img src=\"images/rag_1.png\" alt=\"AIDO.Protein-RAG\" width=\"300\" style=\"background-color:white;\"/>\n",
    "\n",
    "<img src=\"images/rag_2.png\" alt=\"AIDO.Protein-RAG\" width=\"400\" style=\"background-color:white;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3866db09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: torch.Size([12800])\n",
      "tensor([17,  2,  2,  ...,  3, 20,  9], device='cuda:0')\n",
      "pos_encoding: torch.Size([2, 12800])\n",
      "tensor([[  0,   1,   2,  ..., 214, 215, 216],\n",
      "        [  0,   0,   0,  ...,  40,  40,  40]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def tokenize(q_seq, msa, tokenizer, max_context=12800):\n",
    "    \"\"\"\n",
    "    Tokenizes the input sequence and optionally additional sequences for multiple sequence alignment (MSA).\n",
    "    \n",
    "    Args:\n",
    "        q_seq (str): The query sequence to be tokenized.\n",
    "        msa (list or None): A list of sequences for multiple sequence alignment. If None, no MSA sequences are added.\n",
    "        tokenizer (object): The tokenizer object used to encode the sequences.\n",
    "        max_context (int, optional): The maximum number of tokens to consider in the context. Defaults to 12800.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - tokens (np.ndarray): The tokenized sequences.\n",
    "            - pos_encoding (np.ndarray): The positional encoding for the tokens.\n",
    "    \"\"\"\n",
    "    len_seq = len(q_seq)\n",
    "    tokens = tokenizer.encode(q_seq, add_special_tokens=False)\n",
    "    num_seq = 1\n",
    "    \n",
    "    for msa_seq in msa:\n",
    "        assert len(msa_seq) == len_seq, f\"len(msa_seq)={len(msa_seq)}, len_seq={len_seq}\"\n",
    "        tokens.extend(tokenizer.encode(msa_seq, add_special_tokens=False))\n",
    "        num_seq += 1\n",
    "    \n",
    "    pos_encoding = np.stack([ np.tile(np.arange(len_seq), num_seq), np.repeat(np.arange(num_seq), len_seq) ])\n",
    "    \n",
    "    tokens = np.array(tokens)\n",
    "    tok_mask = (tokens != tokenizer._token_to_id['-'])\n",
    "    tokens, pos_encoding = tokens[tok_mask][:max_context], pos_encoding[..., tok_mask][..., :max_context]\n",
    "    return tokens, pos_encoding\n",
    "\n",
    "tokens, pos_encoding = tokenize(query_sequence, f_msa, tokenizer, max_context=12800)\n",
    "tokens       = torch.from_numpy(tokens).cuda()\n",
    "pos_encoding = torch.from_numpy(pos_encoding).cuda()\n",
    "\n",
    "print(f\"tokens: {tokens.shape}\\n{tokens}\")\n",
    "print(f\"pos_encoding: {pos_encoding.shape}\\n{pos_encoding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37250713",
   "metadata": {},
   "source": [
    "#### Padding the structure embeddings to match token length\n",
    "\n",
    "Sequence embedding and structure embedding will be added element-wise. We need to pad the structure embedding to the same length as the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33d6d00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "str_embs: torch.Size([360, 384])\n",
      "tokens: torch.Size([12800])\n",
      "str_embs: torch.Size([12800, 384])\n"
     ]
    }
   ],
   "source": [
    "print(f\"str_embs: {str_embs.shape}\")\n",
    "print(f\"tokens: {tokens.shape}\")\n",
    "\n",
    "padding = tokens.shape[0]-str_embs.shape[0]\n",
    "str_embs = F.pad(str_embs, (0, 0, 0, padding))\n",
    "\n",
    "print(f\"str_embs: {str_embs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "061baf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_hidden_state: torch.Size([1, 12800, 2304])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): # , torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "    lm_output = model(\n",
    "        input_ids=tokens[None],\n",
    "        position_ids=pos_encoding[None],\n",
    "        inputs_str_embeds=str_embs[None],\n",
    "        output_hidden_states=True,\n",
    "    )\n",
    "    last_hidden_state = lm_output.hidden_states[-1]\n",
    "\n",
    "print(f\"last_hidden_state: {last_hidden_state.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e057fb",
   "metadata": {},
   "source": [
    "## ModelGenerator tasks\n",
    "\n",
    "* **Get embeddings**: input sequence, get per-residue and per-sequence embeddings.\n",
    "* **Sequence level classification**: input sequence, get classification label (e.g., enzyme/non-enzyme).\n",
    "* **Token level classification**: input sequence, get per-residue labels (e.g., secondary structure).\n",
    "* **Sequence level regression**: input sequence, get a real-valued output (e.g., stability).\n",
    "\n",
    "```python\n",
    "from modelgenerator.tasks import Embed\n",
    "from modelgenerator.tasks import SequenceClassification\n",
    "from modelgenerator.tasks import TokenClassification\n",
    "from modelgenerator.tasks import SequenceRegression\n",
    "```\n",
    "\n",
    "### How to implement these tasks using ModelGenerator?\n",
    "* **Backbone**: use `genbio-ai/AIDO.Protein-RAG-16B` as the backbone model.\n",
    "* **Adaptors**: different adaptors can be used for different tasks.\n",
    "* **Dataset**: different datasets can be used for different tasks.\n",
    "* **Loss functions**: different loss functions can be used for different tasks.\n",
    "\n",
    "The following section explains how to use the predefined task class in ModelGenerator to load the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71d8ff3",
   "metadata": {},
   "source": [
    "### Embeddings with MSA and Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c66a974",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/genbio/lib/python3.12/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 13/13 [00:09<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences: list of length 1\n",
      "input_ids: torch.Size([1, 1244])\n",
      "attention_mask: torch.Size([1, 50])\n",
      "special_tokens_mask: list of length 1\n",
      "full_attention_mask: list of length 1\n",
      "query_tokens_mask: list of length 1\n",
      "position_ids: list of length 1\n",
      "inputs_str_embeds: list of length 1\n",
      "torch.Size([1, 50, 2304])\n"
     ]
    }
   ],
   "source": [
    "import os, sys, pathlib, torch\n",
    "os.environ['HF_HOME'] = '/tmp/hf_cache'\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from modelgenerator.tasks import Embed\n",
    "\n",
    "model = Embed.from_config({\"model.backbone\": \"aido_protein_rag_16b\"}).eval()\n",
    "model.backbone.max_length = 12800\n",
    "restypes = 'ARNDCQEGHILKMFPSTWYV'\n",
    "data = {\n",
    "    'sequences': [''.join(random.choice(restypes) for _ in range(50))],\n",
    "    'msa': [ [ ''.join(random.choice(restypes+'-') for _ in range(50)) for _ in range(25) ] ],\n",
    "    'str_emb': np.random.normal(size=(1, 50, 384))\n",
    "}\n",
    "transformed_batch = model.transform(data)\n",
    "for k, v in transformed_batch.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        print(f\"{k}: {v.shape}\")\n",
    "    elif isinstance(v, list):\n",
    "        print(f\"{k}: list of length {len(v)}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    embedding = model(transformed_batch)\n",
    "\n",
    "print(embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca027c1c",
   "metadata": {},
   "source": [
    "### Sequence Level Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41de3198",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 13/13 [00:08<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0039, 0.2004]])\n",
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from modelgenerator.tasks import SequenceClassification\n",
    "\n",
    "model = SequenceClassification.from_config({\"model.backbone\": \"aido_protein_rag_16b\", \"model.n_classes\": 2}).eval()\n",
    "model.backbone.max_length = 12800\n",
    "restypes = 'ARNDCQEGHILKMFPSTWYV'\n",
    "data = {\n",
    "    'sequences': [''.join(random.choice(restypes) for _ in range(50))],\n",
    "    'msa': [ [ ''.join(random.choice(restypes+'-') for _ in range(50)) for _ in range(25) ] ],\n",
    "    'str_emb': np.random.normal(size=(1, 50, 384))\n",
    "}\n",
    "transformed_batch = model.transform(data)\n",
    "with torch.no_grad():\n",
    "    logits = model(transformed_batch)\n",
    "\n",
    "print(logits)\n",
    "print(torch.argmax(logits, dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8178c420",
   "metadata": {},
   "source": [
    "### Token Level Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "931b0529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 13/13 [00:08<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1389, -0.0476, -0.1429],\n",
      "         [ 0.0948, -0.0613,  0.0507],\n",
      "         [ 0.0497, -0.1755, -0.0152],\n",
      "         [-0.0173, -0.0536,  0.1043],\n",
      "         [ 0.0619, -0.0745, -0.0095],\n",
      "         [ 0.0890, -0.0762,  0.0661],\n",
      "         [ 0.1128,  0.0329,  0.0497],\n",
      "         [-0.0100,  0.0207, -0.0390],\n",
      "         [ 0.1484,  0.0014,  0.0852],\n",
      "         [ 0.0504, -0.0116, -0.0466],\n",
      "         [-0.0045,  0.0447, -0.1606],\n",
      "         [ 0.0580,  0.0081, -0.1067],\n",
      "         [ 0.0450,  0.0838, -0.0925],\n",
      "         [ 0.0133, -0.0746, -0.0793],\n",
      "         [ 0.0226,  0.0126,  0.0038],\n",
      "         [-0.0122,  0.1079, -0.0010],\n",
      "         [ 0.0734,  0.0792,  0.1325],\n",
      "         [-0.0793,  0.0099, -0.0040],\n",
      "         [ 0.0741,  0.0202, -0.0121],\n",
      "         [ 0.1228,  0.0773,  0.0103],\n",
      "         [ 0.0661, -0.0741, -0.1237],\n",
      "         [ 0.1132,  0.0276, -0.0024],\n",
      "         [ 0.0739, -0.0825, -0.0299],\n",
      "         [ 0.0052,  0.0290,  0.0185],\n",
      "         [ 0.0949, -0.0175,  0.0382],\n",
      "         [ 0.1304,  0.0236, -0.0514],\n",
      "         [-0.0193, -0.0810,  0.1421],\n",
      "         [-0.0088,  0.0159,  0.0687],\n",
      "         [-0.0289,  0.0619, -0.1155],\n",
      "         [ 0.0697,  0.0525, -0.0704],\n",
      "         [-0.0817,  0.0271, -0.0450],\n",
      "         [ 0.1389,  0.0734,  0.0343],\n",
      "         [-0.0773,  0.1169, -0.0936],\n",
      "         [ 0.0466,  0.0393, -0.0947],\n",
      "         [ 0.0821,  0.0794,  0.0050],\n",
      "         [-0.1045,  0.0677, -0.0099],\n",
      "         [-0.0134,  0.1443, -0.0996],\n",
      "         [ 0.0270, -0.0012, -0.1437],\n",
      "         [ 0.1658, -0.0281, -0.0972],\n",
      "         [ 0.0992, -0.0048, -0.0661],\n",
      "         [-0.0086,  0.0067, -0.0283],\n",
      "         [-0.0982,  0.0107, -0.0800],\n",
      "         [ 0.0714, -0.0353, -0.0079],\n",
      "         [ 0.0525, -0.1068, -0.0311],\n",
      "         [-0.0419, -0.0105, -0.1931],\n",
      "         [ 0.0751, -0.0265,  0.0138],\n",
      "         [ 0.0784, -0.1476, -0.0192],\n",
      "         [ 0.1043, -0.0891, -0.0273],\n",
      "         [ 0.0595, -0.0677, -0.0355],\n",
      "         [ 0.0529, -0.0130, -0.0225]]])\n",
      "tensor([[0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 2, 1, 0, 0, 0, 0, 0, 1,\n",
      "         0, 0, 2, 2, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
      "         0, 0]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from modelgenerator.tasks import TokenClassification\n",
    "model = TokenClassification.from_config({\"model.backbone\": \"aido_protein_rag_16b\", \"model.n_classes\": 3}).eval()\n",
    "model.backbone.max_length = 12800\n",
    "restypes = 'ARNDCQEGHILKMFPSTWYV'\n",
    "data = {\n",
    "    'sequences': [''.join(random.choice(restypes) for _ in range(50))],\n",
    "    'msa': [ [ ''.join(random.choice(restypes+'-') for _ in range(50)) for _ in range(25) ] ],\n",
    "    'str_emb': np.random.normal(size=(1, 50, 384))\n",
    "}\n",
    "transformed_batch = model.transform(data)\n",
    "with torch.no_grad():\n",
    "    logits = model(transformed_batch)\n",
    "\n",
    "print(logits)\n",
    "print(torch.argmax(logits, dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363ec38e",
   "metadata": {},
   "source": [
    "### Seq Level Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28a85a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/genbio/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Metric `SpearmanCorrcoef` will save all targets and predictions in the buffer. For large datasets, this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Loading checkpoint shards: 100%|██████████| 13/13 [00:09<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "from modelgenerator.tasks import SequenceRegression\n",
    "model = SequenceRegression.from_config({\"model.backbone\": \"aido_protein_rag_16b\"}).eval()\n",
    "model.backbone.max_length = 12800\n",
    "restypes = 'ARNDCQEGHILKMFPSTWYV'\n",
    "data = {\n",
    "    'sequences': [''.join(random.choice(restypes) for _ in range(50))],\n",
    "    'msa': [ [ ''.join(random.choice(restypes+'-') for _ in range(50)) for _ in range(25) ] ],\n",
    "    'str_emb': np.random.normal(size=(1, 50, 384))\n",
    "}\n",
    "transformed_batch = model.transform(data)\n",
    "with torch.no_grad():\n",
    "    logits = model(transformed_batch)\n",
    "\n",
    "print(logits.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genbio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
