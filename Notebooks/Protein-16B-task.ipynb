{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd9813ed",
   "metadata": {},
   "source": [
    "# AIDO.Protein-16B: Training with ModelGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79b0ddc",
   "metadata": {},
   "source": [
    "## ProteinGym Supervised DMS Benchmark\n",
    "The Deep Mutational Scanning (DMS) Benchmark in ProteinGym is a comprehensive collection of 283 standardized DMS assays, comprising more than 2.7 million mutated protein sequences from over 200 diverse protein families. These assays capture a wide range of functional properties, such as ligand binding, thermostability, viral replication, drug resistance, and more. The dataset spans diverse taxa (humans, other eukaryotes, prokaryotes, and viruses) and includes a variety of mutation types, such as single amino acid substitutions and indels (insertions or deletions). The primary goal of the DMS Benchmark is to model protein fitness landscapes, which represent the relationship between genetic mutations and their effects on protein fitness or functionality.\n",
    "\n",
    "We finetune the [AIDO.Protein-16B](https://huggingface.co/genbio-ai/AIDO.DNA-16B) and [AIDO.Protein-16B-v1](https://huggingface.co/genbio-ai/AIDO.DNA-16B-v1) models on the DMS benmark.\n",
    "AIDO.ModelGenerator implements both Linear Probing and LoRA finetuning, following a 5-fold cross-validation scheme with the random split strategy proposed in the original [ProteinGym paper](https://www.biorxiv.org/content/10.1101/2023.12.07.570727v1). ModelGenerator also implements both DDP and FSDP for efficient finetuning.\n",
    "\n",
    "For tasks involving indels with limited data, we apply Linear Probing, while LoRA is used for substitution tasks and other indel tasks to balance computational efficiency with fine-tuning effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8967cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/genbio/bin/mgen\n"
     ]
    }
   ],
   "source": [
    "! which mgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d9aca38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: mgen [-h] [-c CONFIG] [--print_config[=flags]]\n",
      "            {fit,validate,test,predict} ...\n",
      "\n",
      "Lightning Trainer command line tool\n",
      "\n",
      "options:\n",
      "  -h, --help            Show this help message and exit.\n",
      "  -c CONFIG, --config CONFIG\n",
      "                        Path to a configuration file in json or yaml format.\n",
      "  --print_config[=flags]\n",
      "                        Print the configuration after applying all other\n",
      "                        arguments and exit. The optional flags customizes the\n",
      "                        output and are one or more keywords separated by\n",
      "                        comma. The supported flags are: skip_default,\n",
      "                        skip_null.\n",
      "\n",
      "subcommands:\n",
      "  For more details of each subcommand, add it as an argument followed by\n",
      "  --help.\n",
      "\n",
      "  Available subcommands:\n",
      "    fit                 Runs the full optimization routine.\n",
      "    validate            Perform one evaluation epoch over the validation set.\n",
      "    test                Perform one evaluation epoch over the test set. It's\n",
      "                        separated from fit to make sure you never run on your\n",
      "    predict             Run inference on your data. This will call the model\n",
      "                        forward function to compute predictions. Useful to\n"
     ]
    }
   ],
   "source": [
    "! mgen --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df794dc0",
   "metadata": {},
   "source": [
    "## Start Training by Command Line\n",
    "\n",
    "### Command to run LoRA finetuning with DDP on 1 nodes, 3 GPUs per node:\n",
    "\n",
    "Config file: [substitution_LoRA_DDP.yaml](../ModelGenerator/experiments/AIDO.Protein/DMS/configs/substitution_LoRA_DDP.yaml)\n",
    "\n",
    "#### Add Logger to yaml:\n",
    "\n",
    "```yaml\n",
    "trainer:\n",
    "  logger:\n",
    "  - class_path: lightning.pytorch.loggers.WandbLogger\n",
    "    init_args:\n",
    "      name: null\n",
    "      project: null\n",
    "```\n",
    "\n",
    "Start training with the command below:\n",
    "\n",
    "```bash\n",
    "export HF_HOME=/tmp/hf_cache\n",
    "\n",
    "TASK_NAME='A4GRB6_PSEAI_Chen_2020'\n",
    "MUTATION_TYPE='singles_substitutions'\n",
    "RUN_NAME=${TASK_NAME}_fold0\n",
    "\n",
    "mgen fit --config experiments/AIDO.Protein/DMS/configs/substitution_LoRA_DDP.yaml \\\n",
    "    --data.train_split_files \"[\\\"${MUTATION_TYPE}/${TASK_NAME}.tsv\\\"]\" \\\n",
    "    --data.cv_test_fold_id 0 \\\n",
    "    --data.batch_size 2 \\\n",
    "    --trainer.logger.name ${RUN_NAME} \\\n",
    "    --trainer.logger.project AIDO_Demo \\\n",
    "    --trainer.num_nodes 1 \\\n",
    "    --trainer.devices auto\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0170a0cc",
   "metadata": {},
   "source": [
    "```\n",
    "wandb: Currently logged in as: hnsfyfyzlp (tsinghua-lipan) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
    "wandb: Tracking run with wandb version 0.22.2\n",
    "wandb: Run data is saved locally in ./wandb/run-20251023_184315-A4GRB6_PSEAI_Chen_2020_fold0\n",
    "wandb: Run `wandb offline` to turn off syncing.\n",
    "wandb: Syncing run A4GRB6_PSEAI_Chen_2020_fold0\n",
    "wandb: ‚≠êÔ∏è View project at https://wandb.ai/tsinghua-lipan/AIDO_Demo\n",
    "wandb: üöÄ View run at https://wandb.ai/tsinghua-lipan/AIDO_Demo/runs/A4GRB6_PSEAI_Chen_2020_fold0\n",
    "Repo card metadata block was not found. Setting CardData to empty.\n",
    "Repo card metadata block was not found. Setting CardData to empty.\n",
    "Repo card metadata block was not found. Setting CardData to empty.\n",
    "singles_substitutions/A4GRB6_PSEAI_Chen_(‚Ä¶): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.43M/1.43M [00:00<00:00, 1.90MB/s]\n",
    "Generating train split: 5004 examples [00:00, 207951.11 examples/s]\n",
    "label: mean = [-1.93724161], std = [2.1050639]\n",
    "/opt/miniconda/envs/genbio/lib/python3.12/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
    "  warnings.warn(\n",
    "/opt/miniconda/envs/genbio/lib/python3.12/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
    "  warnings.warn(\n",
    "/opt/miniconda/envs/genbio/lib/python3.12/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
    "  warnings.warn(\n",
    "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:17<00:00,  1.37s/it]\n",
    "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:17<00:00,  1.37s/it]\n",
    "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:18<00:00,  1.39s/it]\n",
    "trainable params: 11,948,544 || all params: 16,071,144,192 || trainable%: 0.0743\n",
    "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
    "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
    "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
    "\n",
    "  | Name     | Type                | Params | Mode \n",
    "---------------------------------------------------------\n",
    "0 | metrics  | ModuleDict          | 0      | train\n",
    "1 | loss     | MSELoss             | 0      | train\n",
    "2 | backbone | aido_protein_16b_v1 | 16.1 B | train\n",
    "3 | adapter  | MLPPoolAdapter      | 295 K  | train\n",
    "---------------------------------------------------------\n",
    "12.2 M    Trainable params\n",
    "16.1 B    Non-trainable params\n",
    "16.1 B    Total params\n",
    "64,285.757Total estimated model params size (MB)\n",
    "1829      Modules in train mode\n",
    "1160      Modules in eval mode\n",
    "Sanity Checking: |                                                                                                                                                                  | 0/? [00:00<?, ?it/s]/opt/miniconda/envs/genbio/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=68` in the `DataLoader` to improve performance.\n",
    "[rank1]:[W1023 18:43:42.316982181 CPUAllocator.cpp:245] Memory block of unknown size was allocated before the profiling started, profiler results will not include the deallocation event                 \n",
    "[rank2]:[W1023 18:43:42.317155840 CPUAllocator.cpp:245] Memory block of unknown size was allocated before the profiling started, profiler results will not include the deallocation event\n",
    "[rank0]:[W1023 18:43:42.319645894 CPUAllocator.cpp:245] Memory block of unknown size was allocated before the profiling started, profiler results will not include the deallocation event\n",
    "/opt/miniconda/envs/genbio/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=68` in the `DataLoader` to improve performance.\n",
    "Epoch 0:   1%|‚ñé                                | 4/511 [00:03<06:24,  1.32it/s, v_num=old0, train_loss=1.330, train_pearson=1.000, train_spearman=1.000, train_mae=1.150, train_r2=-622., train_mse=1.330][rank0]:[W1023 18:43:48.316836962 collection.cpp:647] Warning: Optimizer.step#AdamW.step (function operator())\n",
    "[rank2]:[W1023 18:43:48.410975745 collection.cpp:647] Warning: Optimizer.step#AdamW.step (function operator())\n",
    "[rank1]:[W1023 18:43:48.496507396 collection.cpp:647] Warning: Optimizer.step#AdamW.step (function operator())\n",
    "Epoch 0:  24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      | 124/511 [01:16<03:57,  1.63it/s, v_num=old0, train_loss=0.884, train_pearson=1.000, train_spearman=1.000, train_mae=0.693, train_r2=-0.833, train_mse=0.884]\n",
    "```\n",
    "\n",
    "<img src=\"images/16b_training_curve.png\" alt=\"AIDO.Protein\" width=\"80%\" style=\"background-color:white;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d843c717",
   "metadata": {},
   "source": [
    "## Create object step by step in Jupyter Notebook\n",
    "\n",
    "It is very simple to start training through one command, but you may not know how it works. Let's break down the contents of the yaml configuration file.\n",
    "\n",
    "Config file: [substitution_LoRA_DDP.yaml](../ModelGenerator/experiments/AIDO.Protein/DMS/configs/substitution_LoRA_DDP.yaml)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8551b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.__version__: 2.6.0+cu118\n"
     ]
    }
   ],
   "source": [
    "import os, sys, pathlib, torch\n",
    "print(f\"torch.__version__: {torch.__version__}\")\n",
    "os.environ['HF_HOME'] = '/tmp/hf_cache'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc38da67",
   "metadata": {},
   "source": [
    "Dataset: [genbio-ai/ProteinGYM-DMS](https://huggingface.co/datasets/genbio-ai/ProteinGYM-DMS)\n",
    "\n",
    "\n",
    "```yaml\n",
    "data:\n",
    "  class_path: modelgenerator.data.DMSFitnessPrediction\n",
    "  init_args:\n",
    "    path: genbio-ai/ProteinGYM-DMS\n",
    "    train_split_files:\n",
    "    - singles_substitutions/VRPI_BPT7_Tsuboyama_2023_2WNM.tsv\n",
    "    train_split_name: 'train'\n",
    "    random_seed: 42\n",
    "    batch_size: 32\n",
    "    cv_num_folds: 5\n",
    "    cv_test_fold_id: 0\n",
    "    cv_enable_val_fold: true\n",
    "    cv_fold_id_col: fold_id\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66965ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "label: mean = [-0.68593335], std = [0.93930578]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Train:  634\n",
      "#Val:  216\n",
      "#Test:  197\n"
     ]
    }
   ],
   "source": [
    "from modelgenerator.data import DMSFitnessPrediction\n",
    "\n",
    "datamodule = DMSFitnessPrediction(\n",
    "    path='genbio-ai/ProteinGYM-DMS', \n",
    "    train_split_files=['singles_substitutions/VRPI_BPT7_Tsuboyama_2023_2WNM.tsv'],\n",
    "    train_split_name='train', \n",
    "    random_seed=42, \n",
    "    batch_size=4, \n",
    "    num_workers=8,\n",
    "    cv_num_folds=5, \n",
    "    cv_test_fold_id=0, \n",
    "    cv_enable_val_fold=True, \n",
    "    cv_fold_id_col='fold_id')\n",
    "\n",
    "datamodule.setup()\n",
    "\n",
    "print(\"#Train: \", len(datamodule.train_dataset))\n",
    "print(\"#Val: \", len(datamodule.val_dataset))\n",
    "print(\"#Test: \", len(datamodule.test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73187ec6",
   "metadata": {},
   "source": [
    "```yaml\n",
    "model:\n",
    "  class_path: modelgenerator.tasks.SequenceRegression\n",
    "  init_args:\n",
    "    backbone:\n",
    "      class_path: modelgenerator.backbones.aido_protein_16b_v1\n",
    "      init_args:\n",
    "        use_peft: true\n",
    "        max_length: 2048\n",
    "    adapter:\n",
    "      class_path: modelgenerator.adapters.MLPPoolAdapter\n",
    "      init_args:\n",
    "        hidden_sizes:\n",
    "        - 128\n",
    "        dropout: 0.1\n",
    "        dropout_in_middle: false\n",
    "    optimizer:\n",
    "      class_path: torch.optim.AdamW\n",
    "      init_args:\n",
    "        lr: 0.0001\n",
    "        weight_decay: 0.01\n",
    "    lr_scheduler:\n",
    "      class_path: modelgenerator.lr_schedulers.CosineWithWarmup\n",
    "      init_args:\n",
    "        warmup_ratio: 0.05\n",
    "```\n",
    "\n",
    "There is one difference in the initialization of model: backbone, optimizer, adapter and other classes require the task objects to initialize, so we pass in callable object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1344f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/genbio/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Metric `SpearmanCorrcoef` will save all targets and predictions in the buffer. For large datasets, this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from functools import partial\n",
    "from modelgenerator.tasks import SequenceRegression\n",
    "from modelgenerator.backbones import aido_protein_16b_v1\n",
    "from modelgenerator.adapters import MLPPoolAdapter\n",
    "from modelgenerator.lr_schedulers import CosineWithWarmup\n",
    "\n",
    "modelmodule = SequenceRegression(\n",
    "    backbone    = partial(aido_protein_16b_v1, use_peft=True, max_length=2048),\n",
    "    adapter     = partial(MLPPoolAdapter, hidden_sizes=[128], dropout=0.1, dropout_in_middle=False),\n",
    "    optimizer   = partial(torch.optim.AdamW, lr=0.0001, weight_decay=0.01),\n",
    "    lr_scheduler= partial(CosineWithWarmup, warmup_ratio=0.05)\n",
    ")\n",
    "\n",
    "modelmodule.setup(stage='fit')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d49a9a",
   "metadata": {},
   "source": [
    "```yaml\n",
    "trainer:\n",
    "  accelerator: auto\n",
    "  devices: auto\n",
    "  logger: false\n",
    "  callbacks:\n",
    "  - class_path: lightning.pytorch.callbacks.ModelCheckpoint # save ckpt at the end of each epoch, and save the best val_mcc ckpt\n",
    "    init_args:\n",
    "      filename: epoch_{epoch}-val_mcc:{val_spearman:.3f}\n",
    "      monitor: val_spearman\n",
    "      mode: max\n",
    "  - class_path: lightning.pytorch.callbacks.early_stopping.EarlyStopping\n",
    "    dict_kwargs:\n",
    "      monitor: val_spearman\n",
    "      mode: max\n",
    "      patience: 10\n",
    "  max_steps: 10000\n",
    "  gradient_clip_val: 0.1\n",
    "  default_root_dir: logs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44406ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 3 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=3)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "trainer = Trainer(\n",
    "        accelerator=\"auto\", \n",
    "        devices=\"auto\", \n",
    "        logger=False,\n",
    "        callbacks=[\n",
    "            ModelCheckpoint(\n",
    "                filename=\"epoch_{epoch}-val_mcc:{val_spearman:.3f}\",\n",
    "                monitor=\"val_spearman\",\n",
    "                mode=\"max\"\n",
    "            ),\n",
    "            EarlyStopping(\n",
    "                monitor=\"val_spearman\",\n",
    "                mode=\"max\",\n",
    "                patience=10\n",
    "            )\n",
    "        ],\n",
    "        max_steps=10000,\n",
    "        gradient_clip_val=0.1,\n",
    "        default_root_dir=\"logs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe58f212",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "label: mean = [-0.68593335], std = [0.93930578]\n",
      "/opt/miniconda/envs/genbio/lib/python3.12/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 136.00 MiB. GPU 0 has a total capacity of 79.18 GiB of which 44.56 MiB is free. Process 1712574 has 33.05 GiB memory in use. Including non-PyTorch memory, this process has 46.07 GiB memory in use. Of the allocated memory 45.26 GiB is allocated by PyTorch, and 214.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodelmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda/envs/genbio/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:538\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    536\u001b[39m \u001b[38;5;28mself\u001b[39m.state.status = TrainerStatus.RUNNING\n\u001b[32m    537\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m538\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    539\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda/envs/genbio/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:47\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     46\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     50\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda/envs/genbio/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:574\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    568\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    569\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    570\u001b[39m     ckpt_path,\n\u001b[32m    571\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    572\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    573\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m574\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    577\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda/envs/genbio/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:945\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m    943\u001b[39m call._call_setup_hook(\u001b[38;5;28mself\u001b[39m)  \u001b[38;5;66;03m# allow user to set up LightningModule in accelerator environment\u001b[39;00m\n\u001b[32m    944\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: configuring model\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m945\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_configure_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[38;5;66;03m# check if we should delay restoring checkpoint till later\u001b[39;00m\n\u001b[32m    948\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.strategy.restore_checkpoint_after_setup:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda/envs/genbio/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:119\u001b[39m, in \u001b[36m_call_configure_model\u001b[39m\u001b[34m(trainer)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_overridden(\u001b[33m\"\u001b[39m\u001b[33mconfigure_model\u001b[39m\u001b[33m\"\u001b[39m, trainer.lightning_module):\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m trainer.strategy.tensor_init_context(), trainer.strategy.model_sharded_context(), trainer.precision_plugin.module_init_context():  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m         \u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconfigure_model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda/envs/genbio/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:167\u001b[39m, in \u001b[36m_call_lightning_module_hook\u001b[39m\u001b[34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[39m\n\u001b[32m    164\u001b[39m pl_module._current_fx_name = hook_name\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    170\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Demo/ModelGenerator/modelgenerator/tasks/tasks.py:1328\u001b[39m, in \u001b[36mSequenceRegression.configure_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28mself\u001b[39m.adapter = \u001b[38;5;28mself\u001b[39m.backbone.get_decoder()\n\u001b[32m   1327\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1328\u001b[39m     \u001b[38;5;28mself\u001b[39m.backbone = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackbone_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1329\u001b[39m     \u001b[38;5;28mself\u001b[39m.adapter = \u001b[38;5;28mself\u001b[39m.adapter_fn(\n\u001b[32m   1330\u001b[39m         \u001b[38;5;28mself\u001b[39m.backbone.get_embedding_size(), \u001b[38;5;28mself\u001b[39m.num_outputs\n\u001b[32m   1331\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Demo/ModelGenerator/modelgenerator/backbones/backbones.py:380\u001b[39m, in \u001b[36mGenBioFM.__init__\u001b[39m\u001b[34m(self, legacy_adapter_type, default_config, from_scratch, max_length, use_peft, frozen, save_peft_only, lora_r, lora_alpha, lora_dropout, lora_target_modules, lora_modules_to_save, lora_use_rslora, config_overwrites, model_init_args)\u001b[39m\n\u001b[32m    378\u001b[39m     model = model_class(config=config, **\u001b[38;5;28mself\u001b[39m.model_init_args)\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m380\u001b[39m     model = \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_init_args\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_legacy_adapter:\n\u001b[32m    385\u001b[39m     \u001b[38;5;28mself\u001b[39m.encoder = model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda/envs/genbio/lib/python3.12/site-packages/transformers/modeling_utils.py:3375\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[39m\n\u001b[32m   3369\u001b[39m config = \u001b[38;5;28mcls\u001b[39m._autoset_attn_implementation(\n\u001b[32m   3370\u001b[39m     config, use_flash_attention_2=use_flash_attention_2, torch_dtype=torch_dtype, device_map=device_map\n\u001b[32m   3371\u001b[39m )\n\u001b[32m   3373\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[32m   3374\u001b[39m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3375\u001b[39m     model = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3377\u001b[39m \u001b[38;5;66;03m# make sure we use the model's config since the __init__ call might have copied it\u001b[39;00m\n\u001b[32m   3378\u001b[39m config = model.config\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Demo/ModelGenerator/modelgenerator/huggingface_models/fm4bio/modeling_fm4bio.py:1087\u001b[39m, in \u001b[36mFM4BioModel.__init__\u001b[39m\u001b[34m(self, config, add_pooling_layer)\u001b[39m\n\u001b[32m   1085\u001b[39m     \u001b[38;5;28mself\u001b[39m.config.norm_cls = nn.LayerNorm\n\u001b[32m   1086\u001b[39m \u001b[38;5;28mself\u001b[39m.embeddings = FM4BioEmbeddings(config)\n\u001b[32m-> \u001b[39m\u001b[32m1087\u001b[39m \u001b[38;5;28mself\u001b[39m.encoder = \u001b[43mFM4BioEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1089\u001b[39m \u001b[38;5;28mself\u001b[39m.pooler = FM4BioPooler(config) \u001b[38;5;28;01mif\u001b[39;00m add_pooling_layer \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1091\u001b[39m \u001b[38;5;66;03m# rotary position embeddings\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Demo/ModelGenerator/modelgenerator/huggingface_models/fm4bio/modeling_fm4bio.py:738\u001b[39m, in \u001b[36mFM4BioEncoder.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m    736\u001b[39m \u001b[38;5;28mself\u001b[39m.config = config\n\u001b[32m    737\u001b[39m \u001b[38;5;28mself\u001b[39m.layer = nn.ModuleList(\n\u001b[32m--> \u001b[39m\u001b[32m738\u001b[39m     [\u001b[43mFM4BioLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config.num_hidden_layers)]\n\u001b[32m    739\u001b[39m )\n\u001b[32m    741\u001b[39m \u001b[38;5;66;03m# The final layer norm. We removed the 1st LN, moved LN to each hidden layer and this one\u001b[39;00m\n\u001b[32m    742\u001b[39m \u001b[38;5;66;03m# is simply the final LN (Transformer's BERT has it attached to each hidden layer).\u001b[39;00m\n\u001b[32m    743\u001b[39m \u001b[38;5;28mself\u001b[39m.ln = config.norm_cls(config.hidden_size, eps=config.layer_norm_eps)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Demo/ModelGenerator/modelgenerator/huggingface_models/fm4bio/modeling_fm4bio.py:617\u001b[39m, in \u001b[36mFM4BioLayer.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m    615\u001b[39m     \u001b[38;5;28mself\u001b[39m.crossattention = FM4BioAttention(config)\n\u001b[32m    616\u001b[39m \u001b[38;5;28mself\u001b[39m.ln = config.norm_cls(config.hidden_size, eps=config.layer_norm_eps)\n\u001b[32m--> \u001b[39m\u001b[32m617\u001b[39m \u001b[38;5;28mself\u001b[39m.mlp = \u001b[43mFM4BioMLP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[38;5;28mself\u001b[39m.output = FM4BioOutput(config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Demo/ModelGenerator/modelgenerator/huggingface_models/fm4bio/modeling_fm4bio.py:479\u001b[39m, in \u001b[36mFM4BioMLP.__init__\u001b[39m\u001b[34m(self, config, device)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28mself\u001b[39m.experts_per_token = config.experts_per_token  \u001b[38;5;66;03m# 2\u001b[39;00m\n\u001b[32m    478\u001b[39m \u001b[38;5;66;03m# Project to 4h. If using swiglu double the output width, see https://arxiv.org/pdf/2002.05202.pdf\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m \u001b[38;5;28mself\u001b[39m.dense_h_to_4h = \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mintermediate_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_config_to_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mswiglu\u001b[39m(x):\n\u001b[32m    488\u001b[39m     x = torch.chunk(x, \u001b[32m2\u001b[39m, dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda/envs/genbio/lib/python3.12/site-packages/torch/nn/modules/linear.py:106\u001b[39m, in \u001b[36mLinear.__init__\u001b[39m\u001b[34m(self, in_features, out_features, bias, device, dtype)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28mself\u001b[39m.in_features = in_features\n\u001b[32m    104\u001b[39m \u001b[38;5;28mself\u001b[39m.out_features = out_features\n\u001b[32m    105\u001b[39m \u001b[38;5;28mself\u001b[39m.weight = Parameter(\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m )\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[32m    109\u001b[39m     \u001b[38;5;28mself\u001b[39m.bias = Parameter(torch.empty(out_features, **factory_kwargs))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda/envs/genbio/lib/python3.12/site-packages/torch/utils/_device.py:104\u001b[39m, in \u001b[36mDeviceContext.__torch_function__\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs.get(\u001b[33m'\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    103\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.device\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda/envs/genbio/lib/python3.12/site-packages/lightning/fabric/utilities/init.py:53\u001b[39m, in \u001b[36m_EmptyInit.__torch_function__\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m     51\u001b[39m kwargs = kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.enabled:\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(func, \u001b[33m\"\u001b[39m\u001b[33m__module__\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[33m\"\u001b[39m\u001b[33mtorch.nn.init\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mtensor\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 136.00 MiB. GPU 0 has a total capacity of 79.18 GiB of which 44.56 MiB is free. Process 1712574 has 33.05 GiB memory in use. Including non-PyTorch memory, this process has 46.07 GiB memory in use. Of the allocated memory 45.26 GiB is allocated by PyTorch, and 214.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "trainer.fit(modelmodule, datamodule=datamodule, ckpt_path=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genbio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
