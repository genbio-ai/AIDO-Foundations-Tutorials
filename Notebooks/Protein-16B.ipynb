{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea92f5b7",
   "metadata": {},
   "source": [
    "## AIDO.Protein-16B\n",
    "\n",
    "[AIDO.Protein-16B](https://huggingface.co/genbio-ai/AIDO.Protein-16B) is a protein language model, trained on 1.2 trillion amino acids sourced from UniRef90 and ColabFoldDB.\n",
    "\n",
    "By leveraging MoE layers, AIDO.Protein efficiently scales to 16 billion parameters, delivering exceptional performance across a vast variety of tasks in protein sequence understanding and sequence generation. Remarkably, AIDO.Protein demonstrates exceptional capability despite being trained solely on single protein sequences. Across over 280 DMS protein fitness prediction tasks, our model outperforms previous state-of-the-art protein sequence models without MSA and achieves 99% of the performance of models that utilize MSA, highlighting the strength of its learned representations.\n",
    "\n",
    "Reference: [Mixture of Experts Enable Efficient and Effective Protein Understanding and Design](https://www.biorxiv.org/content/10.1101/2024.11.29.625425v1)\n",
    "\n",
    "<img src=\"images/proteinmoe_architecture.png\" alt=\"AIDO.Protein\" width=\"600\" style=\"background-color:white;\"/>\n",
    "\n",
    "| Model Arch Component    | Value |\n",
    "| ----------------------- | :---: |\n",
    "| Num Attention Head      |  36   |\n",
    "| Num Hidden Layer        |  36   |\n",
    "| Hidden Size             | 2304  |\n",
    "| FFN Hidden Size         | 7680  |\n",
    "| Num MoE Layer per Block |   8   |\n",
    "| Num MoE Layer per Token |   2   |\n",
    "| Vocab Size              |  44   |\n",
    "| Context Length          | 2048  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d5e140",
   "metadata": {},
   "source": [
    "## Step-by-Step Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aebce03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelgenerator_path: /jfs/pan-li/Demo/ModelGenerator/modelgenerator\n"
     ]
    }
   ],
   "source": [
    "import os, sys, pathlib, torch\n",
    "os.environ['HF_HOME'] = '/tmp/hf_cache'\n",
    "import modelgenerator\n",
    "modelgenerator_path = str(pathlib.Path(modelgenerator.__file__).parent)\n",
    "print(f\"modelgenerator_path: {modelgenerator_path}\")\n",
    "\n",
    "from modelgenerator.huggingface_models.fm4bio import FM4BioForMaskedLM\n",
    "from modelgenerator.huggingface_models.fm4bio import FM4BioTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14235bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/genbio/lib/python3.12/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 13/13 [00:08<00:00,  1.59it/s]\n"
     ]
    }
   ],
   "source": [
    "model = FM4BioForMaskedLM.from_pretrained(\"genbio-ai/AIDO.Protein-16B\")\n",
    "vocab_file = os.path.join(modelgenerator_path, \"huggingface_models/fm4bio/vocab_protein.txt\")\n",
    "tokenizer = FM4BioTokenizer(vocab_file=vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bc0005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [18, 6, 1, 1, 13, 19, 7, 1, 10, 34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: torch.Size([1, 10, 128])\n",
      "last_hidden_states: torch.Size([1, 10, 2304])\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode('HELLQWRLD', add_special_tokens=True)\n",
    "print(f\"input_ids: {input_ids}\")\n",
    "input_ids = torch.tensor([input_ids])  # Batch size 1\n",
    "\n",
    "with torch.no_grad():\n",
    "    lm_output = model(input_ids, output_hidden_states=True)\n",
    "    logits = lm_output.logits\n",
    "    last_hidden_states = lm_output.hidden_states[-1]\n",
    "\n",
    "print(f\"logits: {logits.shape}\")\n",
    "print(f\"last_hidden_states: {last_hidden_states.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f91ea0",
   "metadata": {},
   "source": [
    "## ModelGenerator tasks\n",
    "\n",
    "* **Get embeddings**: input sequence, get per-residue and per-sequence embeddings.\n",
    "* **Sequence level classification**: input sequence, get classification label (e.g., enzyme/non-enzyme).\n",
    "* **Token level classification**: input sequence, get per-residue labels (e.g., secondary structure).\n",
    "* **Sequence level regression**: input sequence, get a real-valued output (e.g., stability).\n",
    "\n",
    "```python\n",
    "from modelgenerator.tasks import Embed\n",
    "from modelgenerator.tasks import SequenceClassification\n",
    "from modelgenerator.tasks import TokenClassification\n",
    "from modelgenerator.tasks import SequenceRegression\n",
    "```\n",
    "\n",
    "### How to implement these tasks using ModelGenerator?\n",
    "* **Backbone**: use `genbio-ai/AIDO.Protein-16B` as the backbone model.\n",
    "* **Adaptors**: different adaptors can be used for different tasks.\n",
    "* **Dataset**: different datasets can be used for different tasks.\n",
    "* **Loss functions**: different loss functions can be used for different tasks.\n",
    "\n",
    "The following section explains how to use the predefined task class in ModelGenerator to load the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09bd49b",
   "metadata": {},
   "source": [
    "### Get Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73f72f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/genbio/lib/python3.12/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 13/13 [00:08<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences: list of length 2\n",
      "input_ids: torch.Size([2, 6])\n",
      "attention_mask: torch.Size([2, 6])\n",
      "special_tokens_mask: list of length 2\n",
      "torch.Size([2, 6, 2304])\n",
      "tensor([[[ 0.3044,  0.0566,  0.0445,  ..., -0.1136,  0.1243,  0.3385],\n",
      "         [ 0.1187,  0.1608, -0.1112,  ...,  0.0048,  0.0487,  0.2208],\n",
      "         [ 0.2758,  0.1054, -0.0684,  ..., -0.0247, -0.1242,  0.0035],\n",
      "         [ 0.2359,  0.0590,  0.0292,  ...,  0.0735, -0.0834,  0.1099],\n",
      "         [ 0.2090, -0.0955,  0.1451,  ..., -0.0390,  0.1434,  0.0508],\n",
      "         [ 0.1898, -0.0129, -0.0312,  ...,  0.0127, -0.0230,  0.1145]],\n",
      "\n",
      "        [[ 0.3881,  0.0761,  0.0285,  ...,  0.1029,  0.1006,  0.3173],\n",
      "         [ 0.2578, -0.1170,  0.0710,  ...,  0.1311,  0.1170,  0.2706],\n",
      "         [ 0.2240,  0.1821,  0.0016,  ...,  0.0561,  0.1246,  0.1904],\n",
      "         [ 0.2875,  0.1226, -0.0167,  ...,  0.0818, -0.0253,  0.2927],\n",
      "         [ 0.1534,  0.0319,  0.0268,  ...,  0.0514, -0.0410,  0.1762],\n",
      "         [ 0.1700,  0.0045,  0.0832,  ...,  0.0786, -0.0603,  0.1921]]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import os, sys, pathlib, torch\n",
    "os.environ['HF_HOME'] = '/tmp/hf_cache'\n",
    "\n",
    "from modelgenerator.tasks import Embed\n",
    "model = Embed.from_config({\"model.backbone\": \"aido_protein_16b\"}).eval()\n",
    "transformed_batch = model.transform({\"sequences\": [\"HELLQ\", \"WRLD\"]})\n",
    "for k,v in transformed_batch.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        print(f\"{k}: {v.shape}\")\n",
    "    elif isinstance(v, list):\n",
    "        print(f\"{k}: list of length {len(v)}\")\n",
    "\n",
    "embedding = model(transformed_batch)\n",
    "print(embedding.shape)\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5132cb",
   "metadata": {},
   "source": [
    "### Sequence Level Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ff8e91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 13/13 [00:09<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0897,  0.0079, -0.2286],\n",
      "        [ 0.0525, -0.0608, -0.1076]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from modelgenerator.tasks import SequenceClassification\n",
    "model = SequenceClassification.from_config({\"model.backbone\": \"aido_protein_16b\", \"model.n_classes\": 3}).eval()\n",
    "transformed_batch = model.transform({\"sequences\": [\"HELLQ\", \"WRLD\"]})\n",
    "logits = model(transformed_batch)\n",
    "print(logits)\n",
    "print(torch.argmax(logits, dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b99bf4",
   "metadata": {},
   "source": [
    "### Token Level Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "349d9620",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 13/13 [00:14<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0158, -0.0290, -0.0166],\n",
      "         [ 0.0178, -0.0479, -0.0003],\n",
      "         [-0.0464,  0.0480,  0.0859],\n",
      "         [-0.1204,  0.0147,  0.0833],\n",
      "         [ 0.0230, -0.0509, -0.0480],\n",
      "         [-0.0650,  0.2052, -0.0993]],\n",
      "\n",
      "        [[ 0.0295, -0.0584,  0.0766],\n",
      "         [ 0.1101, -0.1542, -0.0080],\n",
      "         [ 0.0885, -0.0574,  0.1072],\n",
      "         [ 0.0963, -0.1165,  0.0631],\n",
      "         [-0.0354,  0.1781, -0.0608],\n",
      "         [ 0.0476, -0.0687,  0.0973]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[0, 0, 2, 2, 0, 1],\n",
      "        [2, 0, 2, 0, 1, 2]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from modelgenerator.tasks import TokenClassification\n",
    "model = TokenClassification.from_config({\"model.backbone\": \"aido_protein_16b\", \"model.n_classes\": 3}).eval()\n",
    "transformed_batch = model.transform({\"sequences\": [\"HELLQ\", \"WRLD\"]})\n",
    "logits = model(transformed_batch)\n",
    "print(logits)\n",
    "print(torch.argmax(logits, dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2cb675",
   "metadata": {},
   "source": [
    "### Sequence Level Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e75808dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/genbio/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Metric `SpearmanCorrcoef` will save all targets and predictions in the buffer. For large datasets, this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Loading checkpoint shards: 100%|██████████| 13/13 [00:08<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1339],\n",
      "        [0.0609]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from modelgenerator.tasks import SequenceRegression\n",
    "model = SequenceRegression.from_config({\"model.backbone\": \"aido_protein_16b\"}).eval()\n",
    "transformed_batch = model.transform({\"sequences\": [\"HELLQ\", \"WRLD\"]})\n",
    "logits = model(transformed_batch)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb7f19c",
   "metadata": {},
   "source": [
    "The above is how to manually load the model and call the model, but what we really want to do is to let ModelGenerator help us complete these things: load the model, load the dataset, preprocess the data, define the loss function, train, and visualize the loss curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7822cb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genbio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
