{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd59f94d",
   "metadata": {},
   "source": [
    "## AIDO.DNA-300M\n",
    "\n",
    "[AIDO.DNA-300M](https://huggingface.co/genbio-ai/AIDO.DNA-300M) is DNA foundation model trained on 10.6 billion nucleotides from 796 species, enabling genome mining, in silico mutagenesis studies, gene expression prediction, and directed sequence generation.\n",
    "\n",
    "By scaling model depth while maintaining a short context length of 4000 nucleotides, AIDO.DNA shows substantial improvements across a breadth of tasks in functional genomics using transfer learning, sequence generation, and unsupervised annotation of functional elements. Notably, AIDO.DNA outperforms prior encoder-only architectures without new data, suggesting that new scaling laws are needed to achieve compute-optimal DNA language models.\n",
    "\n",
    "<img src=\"images/DNA_300M.png\" alt=\"DNA_300M\" width=\"80%\" style=\"background-color:white;\"/>\n",
    "\n",
    "| Model Arch Component        | Value          |\n",
    "| ------------- |:-------------:|\n",
    "| Num Attention Heads      | 32  |\n",
    "| Num Hidden Layers      | 32       |\n",
    "| Hidden Size | 4352       |\n",
    "| Intermediate Size | 11584       |\n",
    "| Vocab Size | 16      |\n",
    "| Context Length | 4000      |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fd3d76",
   "metadata": {},
   "source": [
    "## ModelGenerator tasks\n",
    "\n",
    "* **Get embeddings**: input sequence, get per-residue and per-sequence embeddings.\n",
    "* **Sequence level classification**: input sequence, get classification label (e.g., enzyme/non-enzyme).\n",
    "* **Token level classification**: input sequence, get per-residue labels (e.g., secondary structure).\n",
    "* **Sequence level regression**: input sequence, get a real-valued output (e.g., stability).\n",
    "\n",
    "```python\n",
    "from modelgenerator.tasks import Embed\n",
    "from modelgenerator.tasks import SequenceClassification\n",
    "from modelgenerator.tasks import TokenClassification\n",
    "from modelgenerator.tasks import SequenceRegression\n",
    "```\n",
    "\n",
    "### How to implement these tasks using ModelGenerator?\n",
    "* **Backbone**: use `genbio-ai/AIDO.DNA-300M` as the backbone model.\n",
    "* **Adaptors**: different adaptors can be used for different tasks.\n",
    "* **Dataset**: different datasets can be used for different tasks.\n",
    "* **Loss functions**: different loss functions can be used for different tasks.\n",
    "\n",
    "The following section explains how to use the predefined task class in ModelGenerator to load the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f186070b",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21e9f644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/genbio/lib/python3.12/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You didn't set a max_length for the data in the downstream task\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 1024])\n",
      "tensor([[[-0.1211, -1.1656,  0.1341,  ..., -0.3062,  0.7482, -0.5316],\n",
      "         [-0.3436, -0.7332, -0.4796,  ..., -0.5184, -0.1882, -1.0675],\n",
      "         [-0.1117, -1.3768,  0.3410,  ..., -0.2125,  0.2736, -0.7600],\n",
      "         [ 0.2689, -1.0567,  0.2963,  ...,  0.3879,  0.5795, -0.5904],\n",
      "         [-0.3094, -0.3091,  0.0409,  ...,  0.4478,  0.8100, -0.2500],\n",
      "         [-0.0301, -0.4987,  0.6923,  ...,  0.1198,  0.3909, -0.7629]],\n",
      "\n",
      "        [[-0.1009, -0.8940,  0.6659,  ..., -0.2676,  0.1204, -0.1840],\n",
      "         [-0.0800, -0.5025, -0.2760,  ..., -0.6894, -0.2624, -0.6581],\n",
      "         [ 0.4181, -0.4056,  0.2300,  ...,  0.1285,  0.6482, -0.2483],\n",
      "         [ 0.2937, -0.8096,  0.5548,  ...,  0.2902, -0.1164, -0.1896],\n",
      "         [-0.1779, -0.2603,  0.3952,  ...,  0.0972,  0.5229, -0.3522],\n",
      "         [ 0.0921, -0.5491,  0.7479,  ...,  0.1172, -0.0194, -0.7005]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello world\")\n",
    "import os, sys, pathlib, torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "os.environ['HF_HOME'] = '/tmp/hf_cache'\n",
    "\n",
    "from modelgenerator.tasks import Embed\n",
    "model = Embed.from_config({\"model.backbone\": \"aido_dna_300m\"}).eval()\n",
    "transformed_batch = model.transform({\"sequences\": [\"ACGT\", \"AGCT\"]})\n",
    "embedding = model(transformed_batch)\n",
    "print(embedding.shape)\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764be11c",
   "metadata": {},
   "source": [
    "### Sequence Level Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4f56c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You didn't set a max_length for the data in the downstream task\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4520, 0.2174],\n",
      "        [0.2696, 0.2643]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from modelgenerator.tasks import SequenceClassification\n",
    "model = SequenceClassification.from_config({\"model.backbone\": \"aido_dna_300m\", \"model.n_classes\": 2}).eval()\n",
    "transformed_batch = model.transform({\"sequences\": [\"ACGT\", \"AGCT\"]})\n",
    "logits = model(transformed_batch)\n",
    "print(logits)\n",
    "print(torch.argmax(logits, dim=-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983b3417",
   "metadata": {},
   "source": [
    "#### Token Level Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54d6f8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You didn't set a max_length for the data in the downstream task\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2240, -0.2785, -0.2847],\n",
      "         [-0.3276, -0.3235,  0.0136],\n",
      "         [ 0.1009, -0.6764, -0.0263],\n",
      "         [-0.2982, -0.2781, -0.1608],\n",
      "         [-0.2490, -0.4951, -0.0656],\n",
      "         [ 0.0326, -0.6318, -0.0705]],\n",
      "\n",
      "        [[-0.0281, -0.1438, -0.2070],\n",
      "         [-0.2562, -0.2852,  0.1643],\n",
      "         [-0.0965, -0.0467, -0.2549],\n",
      "         [ 0.1097, -0.6410,  0.1017],\n",
      "         [-0.2554, -0.3513,  0.0117],\n",
      "         [-0.0156, -0.4872, -0.1562]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[0, 2, 0, 2, 2, 0],\n",
      "        [0, 2, 1, 0, 2, 0]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from modelgenerator.tasks import TokenClassification\n",
    "model = TokenClassification.from_config({\"model.backbone\": \"aido_dna_300m\", \"model.n_classes\": 3}).eval()\n",
    "transformed_batch = model.transform({\"sequences\": [\"ACGT\", \"AGCT\"]})\n",
    "logits = model(transformed_batch)\n",
    "print(logits)\n",
    "print(torch.argmax(logits, dim=-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3f5721",
   "metadata": {},
   "source": [
    "#### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d5cc05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/genbio/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Metric `SpearmanCorrcoef` will save all targets and predictions in the buffer. For large datasets, this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/opt/miniconda/envs/genbio/lib/python3.12/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You didn't set a max_length for the data in the downstream task\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0645],\n",
      "        [1.0317]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from modelgenerator.tasks import SequenceRegression\n",
    "model = SequenceRegression.from_config({\"model.backbone\": \"aido_dna_300m\"}).eval()\n",
    "transformed_batch = model.transform({\"sequences\": [\"ACGT\", \"AGCT\"]})\n",
    "logits = model(transformed_batch)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d18a8a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genbio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
